{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact Removal Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from tf_model import make_model\n",
    "from torchinfo import summary\n",
    "\n",
    "# model = make_model(30, 30, N=2)\n",
    "# print(summary(model, input_size=[(32, 30, 120),(32, 30, 120),(32, 120, 120),(32, 120, 120)], col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig\n",
    "\n",
    "art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "artcls_config.save_pretrained(\"artcls-config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTConfig {\n",
      "  \"N\": 2,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_model\": 128,\n",
      "  \"dropout\": 0.1,\n",
      "  \"h\": 8,\n",
      "  \"model_type\": \"ART\",\n",
      "  \"src_channel_size\": 30,\n",
      "  \"tgt_channel_size\": 30,\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "ARTEncoder_CLSConfig {\n",
      "  \"N\": 2,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_model\": 128,\n",
      "  \"dropout\": 0.1,\n",
      "  \"h\": 8,\n",
      "  \"model_type\": \"ARTEncoder_CLSConfig\",\n",
      "  \"src_channel_size\": 30,\n",
      "  \"tgt_channel_size\": 2,\n",
      "  \"time_len\": 1024,\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "print(test_config)\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig.from_pretrained(\"artcls-config\")\n",
    "print(artcls_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Model Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\共用雲端硬碟\\CNElab_陳昱祺\\multi-modal\\FirstMultiModel/EEGART\\tf_model.py:352: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "cls_model = ARTCLSModel(artcls_config)\n",
    "cls_pretrain = ART_CLS_PreTrain(artcls_config)\n",
    "# resumeLoc = './ART/model/ART/modelsave/checkpoint.pth.tar'\n",
    "# # 2. load model\n",
    "# checkpoint = torch.load(resumeLoc)\n",
    "# art_model.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# art_model.save_pretrained('test_config-art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "AutoConfig.register(\"ART\", ARTConfig)\n",
    "AutoModel.register(ARTConfig, ARTModel)\n",
    "\n",
    "AutoConfig.register(\"ARTEncoder_CLSConfig\", ARTEncoder_CLSConfig)\n",
    "AutoModel.register(ARTEncoder_CLSConfig, ARTCLSModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('test_config-art')\n",
    "# 加載目標模型\n",
    "target_model = AutoModel.from_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weight of Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加載來源模型\n",
    "source_model = AutoModel.from_pretrained('test_config-art')\n",
    "\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "encoder_weights = source_model.model.encoder.state_dict()\n",
    "src_expandcov_weights = source_model.model.src_embed.state_dict()\n",
    "\n",
    "# 加載目標模型\n",
    "target_model = ARTCLSModel(artcls_config)\n",
    "\n",
    "# 將 Encoder 的權重加載到目標模型的 Encoder\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "target_model.model.encoder.load_state_dict(encoder_weights)\n",
    "target_model.model.src_embed.load_state_dict(src_expandcov_weights)\n",
    "\n",
    "print(\"Encoder weights successfully transferred!\")\n",
    "target_model.save_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_model import ARTCLSModel\n",
    "from torchinfo import summary\n",
    "\n",
    "# model = ARTCLSModel.from_pretrained('artcls-config')\n",
    "\n",
    "# print(summary(art_model, input_size=[(32, 30, 1024),(32, 30, 1024),(32,1024,1024),(32,1024,1024)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))\n",
    "\n",
    "print(summary(cls_model, input_size=[(32, 30, 1024),(32,1024,1024)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7003, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7265, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 模擬輸入數據\n",
    "src = torch.randn(32, 30, 1024)  # shape: (32, 30, 1024)\n",
    "src_mask = torch.randn(32, 1024, 1024)  # shape: (32, 1024, 1024)\n",
    "label = torch.randint(0, 2, (32,))\n",
    "\n",
    "# 假設你的設備是 GPU\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 移動數據到 GPU\n",
    "src = src.to(device)\n",
    "src_mask = src_mask.to(device)\n",
    "cls_pretrain = cls_pretrain.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "output = cls_model(src, None)\n",
    "logits = output.last_hidden_state.squeeze(dim=1)  # shape: [32, 2]\n",
    "# print(output.last_hidden_state.shape)\n",
    "loss_fct = CrossEntropyLoss()\n",
    "loss = loss_fct(logits, label)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = cls_pretrain(src, None, label)\n",
    "\n",
    "print(loss.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "model.generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模擬自定義 Dataset\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_dim, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 隨機生成數據\n",
    "        self.data = torch.randn(num_samples, seq_len, input_dim)  # 模擬 src\n",
    "        self.masks = torch.randn(num_samples, input_dim, input_dim)  # 模擬 src_mask\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # 模擬 label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.data[idx], \n",
    "            \"src_mask\": self.masks[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 模擬數據集參數\n",
    "train_dataset = MockDataset(num_samples=1000, seq_len=30, input_dim=1024, num_classes=2)\n",
    "eval_dataset = MockDataset(num_samples=200, seq_len=30, input_dim=1024, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample src shape: torch.Size([30, 1024])\n",
      "Sample src_mask shape: torch.Size([1024, 1024])\n",
      "Sample label: tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# 檢查數據集中的一個樣本\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample src shape:\", sample[\"src\"].shape)       # (30, 1024)\n",
    "print(\"Sample src_mask shape:\", sample[\"src_mask\"].shape)  # (1024, 1024)\n",
    "print(\"Sample label:\", sample[\"label\"])              # 標籤值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 124/375 [00:05<00:11, 21.85it/s]\n",
      " 34%|███▍      | 127/375 [00:06<00:22, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.746819019317627, 'eval_accuracy': 0.515, 'eval_f1': 0.5570776255707762, 'eval_runtime': 0.4518, 'eval_samples_per_second': 442.655, 'eval_steps_per_second': 55.332, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [00:12<00:05, 21.60it/s]\n",
      " 67%|██████▋   | 253/375 [00:12<00:11, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7530338168144226, 'eval_accuracy': 0.525, 'eval_f1': 0.5581395348837209, 'eval_runtime': 0.4595, 'eval_samples_per_second': 435.272, 'eval_steps_per_second': 54.409, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 373/375 [00:18<00:00, 21.71it/s]\n",
      "100%|██████████| 375/375 [00:18<00:00, 19.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.75572669506073, 'eval_accuracy': 0.515, 'eval_f1': 0.5488372093023256, 'eval_runtime': 0.4333, 'eval_samples_per_second': 461.569, 'eval_steps_per_second': 57.696, 'epoch': 3.0}\n",
      "{'train_runtime': 18.9306, 'train_samples_per_second': 158.474, 'train_steps_per_second': 19.809, 'train_loss': 0.5835503743489583, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.5835503743489583, metrics={'train_runtime': 18.9306, 'train_samples_per_second': 158.474, 'train_steps_per_second': 19.809, 'total_flos': 0.0, 'train_loss': 0.5835503743489583, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate \n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# # 訓練參數\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",       # 儲存模型的目錄\n",
    "#     eval_strategy=\"epoch\",  # 替换 evaluation_strategy\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     save_strategy=\"epoch\",       # 每個 epoch 保存一次模型\n",
    "#     logging_dir=\"./logs\",        # 日誌目錄\n",
    "#     logging_steps=10,\n",
    "# )\n",
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "\n",
    "# 創建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=cls_pretrain,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
