{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact Removal Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from tf_model import make_model\n",
    "from torchinfo import summary\n",
    "\n",
    "# model = make_model(30, 30, N=2)\n",
    "# print(summary(model, input_size=[(32, 30, 120),(32, 30, 120),(32, 120, 120),(32, 120, 120)], col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig\n",
    "\n",
    "art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "artcls_config.save_pretrained(\"artcls-config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "print(test_config)\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig.from_pretrained(\"artcls-config\")\n",
    "print(artcls_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Model Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "cls_model = ARTCLSModel(artcls_config)\n",
    "cls_pretrain = ART_CLS_PreTrain(artcls_config)\n",
    "# resumeLoc = './ART/model/ART/modelsave/checkpoint.pth.tar'\n",
    "# # 2. load model\n",
    "# checkpoint = torch.load(resumeLoc)\n",
    "# art_model.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# art_model.save_pretrained('test_config-art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "AutoConfig.register(\"ART\", ARTConfig)\n",
    "AutoModel.register(ARTConfig, ARTModel)\n",
    "\n",
    "AutoConfig.register(\"ARTEncoder_CLSConfig\", ARTEncoder_CLSConfig)\n",
    "AutoModel.register(ARTEncoder_CLSConfig, ARTCLSModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('test_config-art')\n",
    "# 加載目標模型\n",
    "target_model = AutoModel.from_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weight of Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加載來源模型\n",
    "source_model = AutoModel.from_pretrained('test_config-art')\n",
    "\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "encoder_weights = source_model.model.encoder.state_dict()\n",
    "src_expandcov_weights = source_model.model.src_embed.state_dict()\n",
    "\n",
    "# 加載目標模型\n",
    "target_model = ARTCLSModel(artcls_config)\n",
    "\n",
    "# 將 Encoder 的權重加載到目標模型的 Encoder\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "target_model.model.encoder.load_state_dict(encoder_weights)\n",
    "target_model.model.src_embed.load_state_dict(src_expandcov_weights)\n",
    "\n",
    "print(\"Encoder weights successfully transferred!\")\n",
    "target_model.save_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_model import ARTCLSModel\n",
    "from torchinfo import summary\n",
    "\n",
    "# model = ARTCLSModel.from_pretrained('artcls-config')\n",
    "\n",
    "print(summary(art_model, input_size=[(32, 30, 1024),(32, 30, 1024),(32,1024,1024),(32,1024,1024)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))\n",
    "\n",
    "# print(summary(cls_model, input_size=[(32, 30, 1024),(32,1024,1024)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from tf_model import make_model\n",
    "from torchinfo import summary\n",
    "\n",
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig\n",
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "artcls_config.save_pretrained(\"artcls-config\")\n",
    "\n",
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "artcls_config = ARTEncoder_CLSConfig.from_pretrained(\"artcls-config\")\n",
    "# print(test_config)\n",
    "# print(artcls_config)\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "cls_model = ARTCLSModel(artcls_config)\n",
    "cls_pretrain = ART_CLS_PreTrain(artcls_config)\n",
    "\n",
    "# 模擬輸入數據\n",
    "src = torch.randn(32, 30, 1024)  # shape: (32, 30, 1024)\n",
    "src_mask = torch.randn(32, 1024, 1024)  # shape: (32, 1024, 1024)\n",
    "# label = torch.randint(0, 2, (32,))\n",
    "label = torch.randn(32, 30, 1024)\n",
    "\n",
    "# 假設你的設備是 GPU\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 移動數據到 GPU\n",
    "src = src.to(device)\n",
    "src_mask = src_mask.to(device)\n",
    "cls_pretrain = cls_pretrain.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "\"\"\" ART Classifier Test \"\"\"\n",
    "# output = cls_model(src, None, return_dict = True)\n",
    "# logits = output.last_hidden_state.squeeze(dim=1)  # shape: [32, 2]\n",
    "# print(output.last_hidden_state.shape)\n",
    "# loss_fct = CrossEntropyLoss()\n",
    "# loss = loss_fct(logits, label)\n",
    "# print(loss)\n",
    "\n",
    "# loss = cls_pretrain(src, None, label)\n",
    "# print(loss.loss)\n",
    "\n",
    "\"\"\" ART Test \"\"\"\n",
    "output = art_model(src=src, tgt=src, src_mask=None, tgt_mask=None, labels=label, return_dict = True)\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模擬自定義 ART Classifier Dataset\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_dim, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 隨機生成數據\n",
    "        self.data = torch.randn(num_samples, seq_len, input_dim)  # 模擬 src\n",
    "        self.masks = torch.randn(num_samples, input_dim, input_dim)  # 模擬 src_mask\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # 模擬 label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.data[idx], \n",
    "            \"src_mask\": self.masks[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 模擬數據集參數\n",
    "train_dataset = MockDataset(num_samples=1000, seq_len=30, input_dim=1024, num_classes=2)\n",
    "eval_dataset = MockDataset(num_samples=200, seq_len=30, input_dim=1024, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模擬自定義 ART Model Dataset\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_dim):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 隨機生成數據\n",
    "        self.data = torch.randn(num_samples, seq_len, input_dim)  # 模擬 src\n",
    "        self.masks = torch.randn(num_samples, input_dim, input_dim)  # 模擬 src_mask\n",
    "        self.labels = torch.randn(num_samples, seq_len, input_dim)  # 模擬 label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.data[idx], \n",
    "            \"tgt\": self.data[idx], \n",
    "            \"src_mask\": self.masks[idx],\n",
    "            \"tgt_mask\": self.masks[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 模擬數據集參數\n",
    "train_dataset = MockDataset(num_samples=100, seq_len=30, input_dim=1024)\n",
    "eval_dataset = MockDataset(num_samples=20, seq_len=30, input_dim=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查數據集中的一個樣本\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample src shape:\", sample[\"src\"].shape)       # (30, 1024)\n",
    "print(\"Sample src_mask shape:\", sample[\"src_mask\"].shape)  # (1024, 1024)\n",
    "print(\"Sample label:\", sample[\"label\"])              # 標籤值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ART Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 自定义数据整理器\n",
    "class SignalDataCollator:\n",
    "    def __call__(self, features):\n",
    "        inputs = torch.stack([f[\"src\"] for f in features])\n",
    "        masks = torch.stack([f[\"src_mask\"] for f in features])\n",
    "        labels = torch.stack([f[\"label\"] for f in features])\n",
    "        return_dict = True\n",
    "        return {\"src\": inputs, \n",
    "                \"tgt\":inputs, \n",
    "                \"src_mask\": masks, \n",
    "                \"tgt_mask\": masks, \n",
    "                \"labels\": labels, \n",
    "                \"return_dict\": return_dict}\n",
    "\n",
    "\n",
    "# 自定义评价指标\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, targets = eval_preds\n",
    "    mse = ((predictions - targets) ** 2).mean()\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# 初始化模型和 Trainer\n",
    "trainer = Trainer(\n",
    "    model=art_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=SignalDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate \n",
    "\n",
    "# metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",       # 儲存模型的目錄\n",
    "    eval_strategy=\"epoch\",  # 替换 evaluation_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    # save_strategy=\"epoch\",       # 每個 epoch 保存一次模型\n",
    "    # logging_dir=\"./logs\",        # 日誌目錄\n",
    "    # logging_steps=10,\n",
    ")\n",
    "# training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "\n",
    "# 創建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=art_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
