{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLT Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['314']\n",
      "Reading G:\\共用雲端硬碟\\CNElab_黎承宣&賴璁毅_EEG_ROI\\A.Dataset\\setfile\\314_ICA_DLtrain.fdt\n",
      "Reading 0 ... 77475  =      0.000 ...   302.637 secs...\n",
      "Total dataset size: 151\n",
      "['49']\n",
      "Reading G:\\共用雲端硬碟\\CNElab_黎承宣&賴璁毅_EEG_ROI\\A.Dataset\\setfile\\49_ICA_DLtrain.fdt\n",
      "Reading 0 ... 59760  =      0.000 ...   233.438 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\共用雲端硬碟\\CNElab_陳昱祺\\multi-modal\\Dataloader\\SLT_dataloader.py:56: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  eeg_data = mne.io.read_raw_eeglab(eeg_path, preload=True).get_data()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size: 114\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './Dataloader')\n",
    "from SLT_dataloader import EEGROIDataset, SignalDataCollator\n",
    "\n",
    "# Usage example\n",
    "roi_folder = \"G:\\\\共用雲端硬碟\\\\CNElab_黎承宣&賴璁毅_EEG_ROI\\\\A.Dataset\\\\processed_setfile\"\n",
    "eeg_folder = \"G:\\\\共用雲端硬碟\\\\CNElab_黎承宣&賴璁毅_EEG_ROI\\\\A.Dataset\\\\setfile\"\n",
    "group_file = \"./Dataloader/subject_groups.json\"\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = EEGROIDataset(roi_folder, eeg_folder, group_file, \"test_train\")\n",
    "print(f\"Total dataset size: {len(train_dataset)}\")\n",
    "# Create dataset\n",
    "test_dataset = EEGROIDataset(roi_folder, eeg_folder, group_file, \"test_eval\")\n",
    "print(f\"Total dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc2a1ce2dc24b20924c7f9486aa2f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbad16f3abe4efb8b482d755e0f464c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_mmm': nan, 'eval_runtime': 34.1329, 'eval_samples_per_second': 3.34, 'eval_steps_per_second': 0.088, 'epoch': 1.0}\n",
      "{'train_runtime': 80.0501, 'train_samples_per_second': 1.886, 'train_steps_per_second': 0.05, 'train_loss': 1.897353172302246, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4, training_loss=1.897353172302246, metrics={'train_runtime': 80.0501, 'train_samples_per_second': 1.886, 'train_steps_per_second': 0.05, 'total_flos': 0.0, 'train_loss': 1.897353172302246, 'epoch': 1.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SLT_dataloader import SignalDataCollator\n",
    "\n",
    "from tf_config import SLTConfig\n",
    "from tf_model import SLTModel\n",
    "\n",
    "# 假設你的設備是 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "slt_config = SLTConfig(src_channel_size=30, tgt_channel_size=3*5003, N=2, d_model = 1024)\n",
    "slt_config.save_pretrained(\"test_slt_confit\")\n",
    "\n",
    "slt_model = SLTModel(slt_config)\n",
    "slt_model = slt_model.to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, targets = eval_preds\n",
    "    logits = torch.tensor(predictions, dtype=torch.float32) \n",
    "    labels = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "        print(\"logits contains nan or inf\")\n",
    "    if torch.isnan(labels).any() or torch.isinf(labels).any():\n",
    "        print(\"labels contains nan or inf\")\n",
    "    \n",
    "    loss_fct = nn.MSELoss()\n",
    "    \n",
    "    # Compute the z-scores\n",
    "    logits_mean = torch.mean(logits, dim=0, keepdim=True)\n",
    "    logits_std = torch.std(logits, dim=0, keepdim=True)\n",
    "    logits_norm = (logits - logits_mean) / (logits_std + 1e-10)\n",
    "\n",
    "    labels_mean = torch.mean(labels, dim=0, keepdim=True)\n",
    "    labels_std = torch.std(labels, dim=0, keepdim=True)\n",
    "    labels_norm = (labels - labels_mean) / (labels_std + 1e-10)\n",
    "\n",
    "    if torch.isnan(logits_norm).any() or torch.isinf(logits_norm).any():\n",
    "        print(\"logits_norm contains nan or inf\")\n",
    "    if torch.isnan(labels_norm).any() or torch.isinf(labels_norm).any():\n",
    "        print(\"labels_norm contains nan or inf\")\n",
    "\n",
    "    mse = loss_fct(logits_norm.float(), labels_norm.float())\n",
    "    return {\"mse\": mse.item()}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=48,\n",
    "    per_device_eval_batch_size=48,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 初始化模型和 Trainer\n",
    "trainer = Trainer(\n",
    "    model=slt_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=SignalDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
