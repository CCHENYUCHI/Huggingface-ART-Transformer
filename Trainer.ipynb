{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLT Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './Dataloader')\n",
    "from SLT_dataloader import EEGROIDataset, SignalDataCollator, EEGROI_fft_Dataset, EEGROI_Power_Dataset\n",
    "\n",
    "# Usage example\n",
    "roi_folder = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\test_data\\\\ROI\\\\Desikan_Kilianny_with_3pca\"\n",
    "eeg_folder = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\test_data\\\\ROI\\\\Desikan_Kilianny_with_3pca\"\n",
    "group_file = \"./Dataloader/subject_groups.json\"\n",
    "# segment_file = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\test_data\\\\RO\\I\\Desikan_Kilianny_with_3pca\\\\roi_removal_segment.txt\"\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = EEGROI_fft_Dataset(roi_folder, eeg_folder, group_file, \"train_dataset\" )\n",
    "print(f\"\\n\\n================================================\\n\")\n",
    "print(f\"Total dataset size: {len(train_dataset)}\")\n",
    "print(f\"\\n================================================\\n\\n\")\n",
    "# Create dataset\n",
    "test_dataset = EEGROI_fft_Dataset(roi_folder, eeg_folder, group_file, \"test_eval\")\n",
    "print(f\"\\n================================================\\n\")\n",
    "print(f\"Total eval dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8628\\1026613076.py:5: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  train_path = \"G:\\\\共用雲端硬碟\\CNElab_陳昱祺\\\\source localization\\\\simulate_data\\\\training_data\\\\\"    # dataset_seedsource_5000_1000000.0_20250402_065238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Power shape: torch.Size([60000, 30, 51])\n",
      "Source Power shape: torch.Size([60000, 204, 51])\n",
      "\n",
      "\n",
      "================================================\n",
      "\n",
      "Total dataset size: 60000\n",
      "\n",
      "================================================\n",
      "\n",
      "\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200']\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 125\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 115\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 117\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 166\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 118\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 128\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 115\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 118\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 116\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 118\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 104\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 167\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 114\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 117\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 120\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 120\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 116\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 117\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 129\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 119\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 123\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 116\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 181\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 120\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 119\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 168\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 165\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 149\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 139\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 120\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 116\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 114\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 134\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 113\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 160\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 164\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 119\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 121\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 116\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 159\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 115\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 124\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 152\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 154\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 162\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 153\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 130\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 148\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 155\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 114\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 114\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 149\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 119\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 117\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 157\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 158\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 150\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 151\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 121\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 156\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 111\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 112\n",
      "Total NaN count for subject: 0\n",
      "Total Random count for subject: 163\n",
      "\n",
      "================================================\n",
      "\n",
      "Total eval dataset size: 28740\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './Dataloader')\n",
    "from SLT_dataloader import EEGROIDataset, SignalDataCollator, EEGROI_fft_Dataset, EEGROI_Power_Dataset\n",
    "\n",
    "train_path = \"G:\\\\共用雲端硬碟\\CNElab_陳昱祺\\\\source localization\\\\simulate_data\\\\training_data\\\\\"    # dataset_seedsource_5000_1000000.0_20250402_065238\n",
    "test_path =   \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\test_data\\\\ROI\\\\Desikan_Kilianny_with_3pca\\\\\"\n",
    "group_file = \"./Dataloader/subject_groups.json\"\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = EEGROI_Power_Dataset(train_path)\n",
    "print(f\"\\n\\n================================================\\n\")\n",
    "print(f\"Total dataset size: {len(train_dataset)}\")\n",
    "print(f\"\\n================================================\\n\\n\")\n",
    "# Create dataset\n",
    "test_dataset = EEGROI_fft_Dataset(test_path, test_path, group_file, \"train_dataset\")\n",
    "print(f\"\\n================================================\\n\")\n",
    "print(f\"Total eval dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Power shape: torch.Size([20000, 30, 100])\n",
      "Source Power shape: torch.Size([20000, 204, 100])\n",
      "EEG Power shape: torch.Size([20000, 30, 100])\n",
      "Source Power shape: torch.Size([20000, 204, 100])\n",
      "EEG Power shape: torch.Size([20000, 30, 100])\n",
      "Source Power shape: torch.Size([20000, 204, 100])\n",
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166']\n",
      "EEG Power shape: torch.Size([30, 100])\n",
      "Source Power shape: torch.Size([204, 100])\n",
      "\n",
      "\n",
      "================================================\n",
      "\n",
      "Total dataset size: 83950\n",
      "\n",
      "================================================\n",
      "\n",
      "\n",
      "['167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200']\n",
      "EEG Power shape: torch.Size([30, 100])\n",
      "Source Power shape: torch.Size([204, 100])\n",
      "\n",
      "================================================\n",
      "\n",
      "Total eval dataset size: 4790\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './Dataloader')\n",
    "from SLT_dataloader import EEGROIDataset, SignalDataCollator, EEGROI_fft_Dataset, EEGROI_Power_Dataset, EEGROI_Merge_Dataset\n",
    "\n",
    "data_path_1 = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\simulate_data\\\\dataset_seedsource_2000_10_20250406_034254\\\\\"\n",
    "data_path_2 = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\simulate_data\\\\dataset_seedsource_2000_1_20250406_034323\\\\\"\n",
    "data_path_3 = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\simulate_data\\\\dataset_seedsource_2000_1000000.0_20250406_034224\\\\\"\n",
    "test_path = \"G:\\\\共用雲端硬碟\\\\CNElab_陳昱祺\\\\source localization\\\\test_data\\\\ROI\\\\Desikan_Kilianny_with_3pca\\\\\"\n",
    "group_file = \"./Dataloader/subject_groups.json\"\n",
    "\n",
    "dataset_1 = EEGROI_Power_Dataset(data_path_1)\n",
    "dataset_2 = EEGROI_Power_Dataset(data_path_2)\n",
    "dataset_3 = EEGROI_Power_Dataset(data_path_3)\n",
    "dataset_4 = EEGROI_fft_Dataset(test_path, test_path, group_file, \"train_dataset\")\n",
    "\n",
    "merge_dataset = EEGROI_Merge_Dataset([dataset_1, dataset_2, dataset_3, dataset_4])\n",
    "train_data_len = len(merge_dataset)\n",
    "print(f\"\\n\\n================================================\\n\")\n",
    "print(f\"Total dataset size: {train_data_len}\")\n",
    "print(f\"\\n================================================\\n\\n\")\n",
    "\n",
    "test_dataset = EEGROI_fft_Dataset(test_path, test_path, group_file, \"test_dataset_temp\")\n",
    "test_data_len = len(test_dataset)\n",
    "print(f\"\\n================================================\\n\")\n",
    "print(f\"Total eval dataset size: {test_data_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: richielp700 (richielp700-national-tsing-hua-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>G:\\共用雲端硬碟\\CNElab_陳昱祺\\multi-modal\\wandb\\run-20250428_003004-fd48qith</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/richielp700-national-tsing-hua-university/huggingface/runs/fd48qith' target=\"_blank\">mask_tarin_adding_EEG_into_tgt_</a></strong> to <a href='https://wandb.ai/richielp700-national-tsing-hua-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/richielp700-national-tsing-hua-university/huggingface' target=\"_blank\">https://wandb.ai/richielp700-national-tsing-hua-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/richielp700-national-tsing-hua-university/huggingface/runs/fd48qith' target=\"_blank\">https://wandb.ai/richielp700-national-tsing-hua-university/huggingface/runs/fd48qith</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10968' max='32800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10968/32800 9:09:03 < 18:13:05, 0.33 it/s, Epoch 33.44/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.026100</td>\n",
       "      <td>0.792697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>0.723823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.898300</td>\n",
       "      <td>0.666824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>0.873400</td>\n",
       "      <td>0.625504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>0.592626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>0.816800</td>\n",
       "      <td>0.558791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.533510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.518774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.508843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.774500</td>\n",
       "      <td>0.495502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>0.788600</td>\n",
       "      <td>0.486598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>0.769600</td>\n",
       "      <td>0.468301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1417</td>\n",
       "      <td>0.777700</td>\n",
       "      <td>0.453061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526</td>\n",
       "      <td>0.779900</td>\n",
       "      <td>0.441947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>0.773900</td>\n",
       "      <td>0.423196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.417747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1853</td>\n",
       "      <td>0.766300</td>\n",
       "      <td>0.401257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1962</td>\n",
       "      <td>0.770400</td>\n",
       "      <td>0.385914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2071</td>\n",
       "      <td>0.749800</td>\n",
       "      <td>0.371544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.741000</td>\n",
       "      <td>0.364523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2289</td>\n",
       "      <td>0.735900</td>\n",
       "      <td>0.356011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2398</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>0.354128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2507</td>\n",
       "      <td>0.720400</td>\n",
       "      <td>0.344051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2616</td>\n",
       "      <td>0.720100</td>\n",
       "      <td>0.342734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.721100</td>\n",
       "      <td>0.347596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2834</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>0.345446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2943</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.335611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3052</td>\n",
       "      <td>0.714100</td>\n",
       "      <td>0.341298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3161</td>\n",
       "      <td>0.692600</td>\n",
       "      <td>0.337122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3270</td>\n",
       "      <td>0.674100</td>\n",
       "      <td>0.333772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3379</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.333406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3488</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.326496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3597</td>\n",
       "      <td>0.660400</td>\n",
       "      <td>0.326672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3706</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.325377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3815</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.322772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3924</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.312838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4033</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.320619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4142</td>\n",
       "      <td>0.653400</td>\n",
       "      <td>0.316420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4251</td>\n",
       "      <td>0.645400</td>\n",
       "      <td>0.316235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>0.310375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4469</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.313590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4578</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.310137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4687</td>\n",
       "      <td>0.628400</td>\n",
       "      <td>0.314439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4796</td>\n",
       "      <td>0.633200</td>\n",
       "      <td>0.314748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4905</td>\n",
       "      <td>0.628700</td>\n",
       "      <td>0.312210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5014</td>\n",
       "      <td>0.630900</td>\n",
       "      <td>0.310568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5123</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.308589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5232</td>\n",
       "      <td>0.632900</td>\n",
       "      <td>0.305308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5341</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.305040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.625200</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5559</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.303867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5668</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.298353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5777</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.304733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5886</td>\n",
       "      <td>0.620600</td>\n",
       "      <td>0.302289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5995</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.298037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6104</td>\n",
       "      <td>0.619500</td>\n",
       "      <td>0.301906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6213</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.299245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6322</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.299056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6431</td>\n",
       "      <td>0.616100</td>\n",
       "      <td>0.299164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>0.300476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6649</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.298181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6758</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.300639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6867</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.293645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6976</td>\n",
       "      <td>0.610600</td>\n",
       "      <td>0.300917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7085</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>0.294659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7194</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.292436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7303</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.298915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7412</td>\n",
       "      <td>0.596200</td>\n",
       "      <td>0.293463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7521</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.294336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7630</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.289395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7739</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.293271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7848</td>\n",
       "      <td>0.597200</td>\n",
       "      <td>0.291264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7957</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.290062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8066</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.288580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8175</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>0.294509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8284</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.286927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8393</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>0.291059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8502</td>\n",
       "      <td>0.598700</td>\n",
       "      <td>0.286933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8611</td>\n",
       "      <td>0.597300</td>\n",
       "      <td>0.288503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8720</td>\n",
       "      <td>0.582200</td>\n",
       "      <td>0.286165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8829</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.290739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8938</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.290437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9047</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.285469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9156</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.287428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9265</td>\n",
       "      <td>0.580200</td>\n",
       "      <td>0.284744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9374</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.282383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9483</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>0.285554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9592</td>\n",
       "      <td>0.592400</td>\n",
       "      <td>0.287157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9701</td>\n",
       "      <td>0.584500</td>\n",
       "      <td>0.286274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9810</td>\n",
       "      <td>0.580400</td>\n",
       "      <td>0.282523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9919</td>\n",
       "      <td>0.582600</td>\n",
       "      <td>0.281306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10028</td>\n",
       "      <td>0.578900</td>\n",
       "      <td>0.287387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10137</td>\n",
       "      <td>0.584900</td>\n",
       "      <td>0.287134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10246</td>\n",
       "      <td>0.577400</td>\n",
       "      <td>0.278790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10355</td>\n",
       "      <td>0.578200</td>\n",
       "      <td>0.279834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10464</td>\n",
       "      <td>0.579500</td>\n",
       "      <td>0.282744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10573</td>\n",
       "      <td>0.581400</td>\n",
       "      <td>0.281849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10682</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>0.277315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10791</td>\n",
       "      <td>0.570300</td>\n",
       "      <td>0.286390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.572600</td>\n",
       "      <td>0.280867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from SLT_dataloader import SignalDataCollator, RandonMaskDataCollator\n",
    "\n",
    "from tf_config import SLTConfig\n",
    "from tf_model import SLTModel, SLTModel_ver2\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取当前日期时间，并格式化为 YYYYMMDD\n",
    "current_date = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 替换路径中的日期\n",
    "runname = \"mask_tarin_adding_EEG_into_tgt_\"\n",
    "output_dir = f\"./results_3pca/{runname}_{current_date}\"\n",
    "\n",
    "# 假設你的設備是 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\"\"\"\n",
    "    Parameters:\n",
    "    \n",
    "\"\"\"\n",
    "# src_channel_size = 30\n",
    "# tgt_channel_size = 204\n",
    "# N = 4\n",
    "# source_voxel_time = 51\n",
    "# sensor_time = 51\n",
    "# tgt_d_model = 51\n",
    "# slt_config_fname = \"test_slt_confit\"\n",
    "\n",
    "# 包裝自定義的初始化方法\n",
    "def huggingface_model_init():\n",
    "    \n",
    "    src_channel_size = 30\n",
    "    tgt_channel_size = 234 # atlas size \n",
    "    N = 8\n",
    "    source_voxel_time = 100\n",
    "    sensor_time = 100\n",
    "    tgt_d_model = 512\n",
    "    slt_config_fname = \"test_slt_confit\"\n",
    "    \n",
    "    slt_config = SLTConfig(src_channel_size=src_channel_size, tgt_channel_size=tgt_channel_size, N=N,\n",
    "                           source_voxel_time=source_voxel_time, sensor_time=sensor_time, tgt_d_model=tgt_d_model)\n",
    "    slt_config.save_pretrained(slt_config_fname)\n",
    "    slt_model = SLTModel(slt_config)\n",
    "    slt_model = slt_model.to(device)\n",
    "    for p in slt_model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            torch.nn.init.xavier_uniform_(p)\n",
    "    return slt_model\n",
    "\n",
    "batch_size = 256\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    run_name = runname,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_train_batch_size=batch_size,   # training batch size\n",
    "    per_device_eval_batch_size=batch_size,    # test batch size\n",
    "    eval_steps= int((train_data_len/batch_size)/3),  # eval steps= 1/3 steps of one epoch\n",
    "    eval_accumulation_steps= 4,        \n",
    "    num_train_epochs=100,              # epoch\n",
    "    weight_decay=0.01,                 # \n",
    "    learning_rate = 0.0005,\n",
    "    lr_scheduler_type = \"constant\",\n",
    "    # fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps= int((train_data_len/batch_size)/10),\n",
    "    report_to=\"wandb\",\n",
    "    dataloader_num_workers=8,  # 🚀 Use multiple workers\n",
    "    dataloader_pin_memory=True,  # 🚀 Reduce CPU-GPU data transfer time\n",
    ")\n",
    "\n",
    "# def compute_metrics(eval_preds, batch_size=64):\n",
    "#     predictions, targets = eval_preds\n",
    "    \n",
    "#     loss_fct = nn.MSELoss()\n",
    "#     num_samples = predictions.shape[0]\n",
    "#     mse_sum = 0.0\n",
    "#     num_batches = 0\n",
    "\n",
    "#     for start_idx in range(0, num_samples, batch_size):\n",
    "#         end_idx = min(start_idx + batch_size, num_samples)\n",
    "#         logits_batch = predictions[start_idx:end_idx]\n",
    "#         labels_batch = targets[start_idx:end_idx]\n",
    "#         logits = torch.tensor(logits_batch, dtype=torch.float32)\n",
    "#         labels = torch.tensor(labels_batch, dtype=torch.float32)\n",
    "#         # Compute the z-scores for the batch\n",
    "#         logits_mean = torch.mean(logits, dim=(1, 2), keepdim=True)\n",
    "#         logits_std = torch.std(logits, dim=(1, 2), keepdim=True)\n",
    "#         logits_norm = (logits - logits_mean) / (logits_std)\n",
    "        \n",
    "#         labels_mean = torch.mean(labels, dim=(1, 2), keepdim=True)\n",
    "#         labels_std = torch.std(labels,  dim=(1, 2), keepdim=True)\n",
    "#         labels_norm = (labels - labels_mean) / (labels_std)\n",
    "        \n",
    "#         # Compute batch MSE\n",
    "#         mse_sum += loss_fct(logits_norm, labels_norm).item()\n",
    "#         num_batches += 1\n",
    "\n",
    "#     # Return the average MSE\n",
    "#     avg_mse = mse_sum / num_batches\n",
    "    \n",
    "#     del logits, labels, logits_norm, labels_norm\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     return {\"mse\": avg_mse}\n",
    "\n",
    "def dummy_compute_metrics(eval_preds):\n",
    "    return {}  # Return empty dict\n",
    "\n",
    "# 初始化模型和 Trainer\n",
    "trainer = Trainer(\n",
    "    # model=slt_model,\n",
    "    model_init=huggingface_model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=merge_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=RandonMaskDataCollator(tgt_type=\"add_eeg\"),\n",
    "    # compute_metrics=dummy_compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
